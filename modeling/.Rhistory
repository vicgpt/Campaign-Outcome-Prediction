# model selected for each city is auto
m$m.auto
m %>% accuracy()
m %>% glance()
DTR <- D %>%
filter(Quarter >= yearquarter("2010 Q1"),
Quarter <= yearquarter("2016 Q4"))
# creating the model for different series
m <- DTR %>%
model(m.auto = ETS(Demand),
m.AAM = ETS(Demand ~ error("A") + trend("A") + season("M")),
m.AAdM = ETS(Demand ~ error("A") + trend("Ad") + season("M"))
)
m %>% accuracy()
# getting the auto model with min AIC and BIC
m %>% glance() %>%
filter(.model == 'm.auto')
m$m.auto
# getting forecast for each city
f = m %>%
forecast(h=4) %>%
filter(.model == 'm.auto')
mape = rbind(m %>% accuracy()  %>% filter(.model == "m.auto"),
f %>% accuracy(data = DTE)) %>%
select(Region, State, Purpose, .model, .type, MAPE)
mape
# aggregating train and test value by quarter
train_agg = DTR %>%
index_by(Quarter) %>%
summarise(Demand = sum(Demand))
test_agg = DTE %>%
index_by(Quarter) %>%
summarise(Demand = sum(Demand))
# aggregating train forecast from the model by quarter
sep_train_fcst = m %>%
select(-c(m.AAM, m.AAdM)) %>%
augment() %>%
index_by(Quarter) %>%
summarize(Agg_Demand = sum(Demand), Agg_Fitted = sum(.fitted), Agg_Residuals = sum(.resid))
# calculating mape from train forecast by quarter
sep_train_mape = MAPE(sep_train_fcst$Agg_Residuals, train_agg$Demand)
# aggregating test forecast from the model by quarter
sep_test_fcst = f %>%
index_by(Quarter) %>%
summarize(sep_mean = sum(.mean))
# calculating mape from test forecast by quarter
sep_test_mape = MAPE(sep_test_fcst$sep_mean - test_agg$Demand, test_agg$Demand)
sep_train_mape
sep_test_mape
m_agg = train_agg %>%
model(m.auto = ETS(Demand),
m.AAdM = ETS(Demand ~ error("A") + trend("Ad") + season("M")),
m.AAM = ETS(Demand ~ error("A") + trend("A") + season("M"))
)
m_agg %>% accuracy()
m_agg %>% glance()
m_agg$m.auto[[1]]$fit$spec
m_agg$m.auto[[1]]$fit$par
agg_test_mape
data_all = D %>%
filter(Quarter >= yearquarter("2010 Q1")) %>%
index_by(Quarter) %>%
summarise(Demand = sum(Demand))
m_all = data_all %>%
model(m.auto = ETS(Demand ~ error("A") + trend("N") + season("A")))
all_train_mape = m_all %>% accuracy() %>% select(MAPE)
all_train_mape = all_train_mape[,,1]
all_train_mape
f_all = m_all %>%
forecast(h=4) %>%
filter(.model == 'm.auto')
m_all_fitted = m_all %>%
augment()
f_all %>% autoplot(data_all) %>%
geom_point(data = m_all_fitted, mapping = aes(y = .fitted), color='blue')
all_train_mape
m
m$m.auto
m$m.auto[1]
# getting the auto model with min AIC and BIC
m %>% glance() %>%
filter(.model == 'm.auto')
m$m.auto
m$m.auto[1]
m
m$m.auto[[1]]
m$m.auto[1]
m$m.auto[[1]]
rbind(Region, m$m.auto)
m$Region
rbind(m$Region, m$m.auto)
data_all = D %>%
filter(Quarter >= yearquarter("2010 Q1")) %>%
index_by(Quarter) %>%
summarise(Demand = sum(Demand))
m_all = data_all %>%
model(m.auto = ETS(Demand ~ error("A") + trend("N") + season("A")))
all_train_mape = m_all %>% accuracy() %>% select(MAPE)
all_train_mape = all_train_mape[,,1]
f_all = m_all %>%
forecast(h=4) %>%
filter(.model == 'm.auto')
m_all_fitted = m_all %>%
augment()
f_all %>% autoplot(data_all) %>%
geom_point(data = m_all_fitted, mapping = aes(y = .fitted), color='blue')
f_all = m_all %>%
forecast(h=4) %>%
filter(.model == 'm.auto')
m_all_fitted = m_all %>%
augment()
f_all %>% autoplot(data_all)
f_all = m_all %>%
forecast(h=4) %>%
filter(.model == 'm.auto')
m_all_fitted = m_all %>%
augment()
autoplot(f_all)
f_all %>% autoplot(data_all)
f_all %>% autoplot(data_all)
f_all = m_all %>%
forecast(h=4) %>%
filter(.model == 'm.auto')
autoplot(f_all)
library(fpp3)
library(tseries)
insurance
autoplot(insurance)
insurance %>%
mutate(diff.Q = difference(Quotes),
diff2.Q = difference(diff.Q)) -> D
# Examine Stationarity Visually
D %>% ACF(Quotes) %>%
autoplot() +
labs(title = "Quotes")
D %>% PACF(Quotes) %>%
autoplot() +
labs(title = "Quotes")
D %>% ACF(diff.Q) %>%
autoplot() +
labs(title = "diff.Q")
D %>% PACF(diff.Q) %>%
autoplot() +
labs(title = "diff.Q")
D %>% ACF(diff2.Q) %>%
autoplot() +
labs(title = "diff2.Q")
D %>% PACF(diff2.Q) %>%
autoplot() +
labs(title = "diff2.Q")
# Unit Root Test
D %>% features(Quotes, unitroot_kpss)
D %>% features(diff.Q, unitroot_kpss)
D %>% features(diff2.Q, unitroot_kpss)
D %>% features(Quotes, unitroot_ndiffs)
# ADF Test
D$Quotes%>% adf.test()
D$diff.Q %>%
na.omit() %>%
adf.test()
D$diff2.Q %>%
na.omit() %>%
adf.test()
D %>% gg_tsdisplay(Quotes, plot_type = "partial")
D %>% gg_tsdisplay(diff.Q, plot_type = "partial")
m <- D %>%
model(ma = ARIMA(Quotes),
mg1 = ARIMA(Quotes ~ pdq(2,0,0)),
mg2 = ARIMA(Quotes ~ pdq(2,1,0)),
mlr = TSLM(Quotes ~ TVadverts))
m %>% select(ma) %>% report
m %>% select(mg1) %>% report
m %>% select(mg2) %>% report
m %>% select(mlr) %>% report
# Residual Testing
# auto correlation of residuals  - Chap 5
# p value low, residuals are not correlated as null hypothesis
m %>% augment() %>%
features(.resid, ljung_box, lag = 10)
m %>% select(ma) %>% gg_tsresiduals()
m %>% select(mg1) %>% gg_tsresiduals()
m %>% select(mg2) %>% gg_tsresiduals()
m %>% select(mlr) %>% gg_tsresiduals()
# first five lags have autocorrelations for LR,this suggests the model is not valid
# Examine Information Criteria
m %>% glance()
# Examine Forecasts
m %>% select(ma) %>%
forecast(h=8) %>%
autoplot(D)
# 95% confidence interval is between min and max of the historical data hence this is not a well fitted model
# Correlation between Quotes and TVadverts
plot(Quotes ~ TVadverts, data = D)
cor(D$Quotes, D$TVadverts)
m <- D%>% model(ma = ARIMA(Quotes),
mra = ARIMA(Quotes ~TVadverts))
m %>% glance() %>%
select(.model, AIC, AICc, BIC)
m %>% select(mra) %>% report()
m %>% select(mra) %>% residuals(type='regression') %>%
gg_tsdisplay(difference(.resid), 'partial', lag_max=10)
# ARIMA is fitting automatically the pdq values
# testing the diff residuals we can agree that model is indeed MA(1)
# BEWARE - looking at difference of n_t
# AUTOMATIC FIT IS NOT GIVING A REASONABLE MODEL
# Best will be (2, 1, 0)
m <- D%>% model(ma = ARIMA(Quotes),
mra = ARIMA(Quotes ~TVadverts),
mrag1 = ARIMA(Quotes ~TVadverts + pdq(2, 1, 0)),
mrag2 = ARIMA(Quotes ~TVadverts + pdq(1, 1, 1)))
m %>% glance() %>%
select(.model, AIC, AICc, BIC)
sas
m %>% report()
m <- D%>% model(ma = ARIMA(Quotes),
mra = ARIMA(Quotes ~TVadverts),
mrag1 = ARIMA(Quotes ~TVadverts + pdq(2, 1, 0)),
mrag2 = ARIMA(Quotes ~TVadverts + pdq(1, 1, 1)))
m %>% glance() %>%
select(.model, AIC, AICc, BIC)
m %>% report()
# Correlation between Quotes and TVadverts
plot(Quotes ~ TVadverts, data = D)
cor(D$Quotes, D$TVadverts)
m <- D%>% model(ma = ARIMA(Quotes),
mra = ARIMA(Quotes ~TVadverts))
m %>% glance() %>%
select(.model, AIC, AICc, BIC)
m %>% select(mra) %>% report()
m %>% select(mra) %>% residuals(type='regression') %>%
gg_tsdisplay(difference(.resid), 'partial', lag_max=10)
m %>% select(mrag1) %>% residuals(type='regression') %>%
gg_tsdisplay(difference(.resid), 'partial', lag_max=10)
m %>% select(mrag1) %>% residuals(type='regression') %>%
gg_tsdisplay(difference(.resid), 'partial', lag_max=10)
m <- D%>% model(ma = ARIMA(Quotes),
mra = ARIMA(Quotes ~TVadverts),
mrag1 = ARIMA(Quotes ~TVadverts + pdq(2, 1, 0)),
mrag2 = ARIMA(Quotes ~TVadverts + pdq(1, 1, 1)))
m %>% glance() %>%
select(.model, AIC, AICc, BIC)
m %>% report()
m %>% select(mrag1) %>% residuals(type='regression') %>%
gg_tsdisplay(difference(.resid), 'partial', lag_max=10)
?resuduals
?residuals
m %>%
augment() %>%
features(.resid, ljung_box, lag=8)
m %>% report()
# PREDICTING THE FUTURE
new_advert_data <- new_data(D, 8) %>%
mutate(TVadverts = c(9, 10, 11, 10, 9, 8, 8, 9))
m%>% select(mra) %>%
forecast(new_data=new_advert_data) %>%
autoplot() +
geom_line(x, mapping=aes(y=Quotes))
m%>% select(mra) %>%
forecast(new_data=new_advert_data) %>%
autoplot() +
geom_line(X, mapping=aes(y=Quotes))
m%>% select(mra) %>%
forecast(new_data=new_advert_data) %>%
autoplot() +
geom_line(D, mapping=aes(y=Quotes))
m %>% select(mra) %>% residuals(type='regression')
library(fpp3)
library(tseries)
X <- read.csv("Booking Exercise.csv") %>%
mutate(DATE = ymd(DATE)) %>%
as_tsibble(index = DATE)
Fcst.X <- X %>% filter(DATE >= ymd("2010-08-18"))
X <- X %>% filter(DATE <= ymd("2010-08-17"))
X %>% autoplot(DEMAND)
X %>% features(DEMAND, unitroot_ndiffs)
X %>% features(DEMAND, unitroot_nsdiffs)
m <- X%>% model(ets = ETS(DEMAND),
arima1 = ARIMA(DEMAND),
lr = TSLM(DEMAND ~ TUESDAY.BOOK),
arima2 = ARIMA(DEMAND ~ TUESDAY.BOOK + DOW.INDEX + PDQ(0, 0, 0)),
arima3 = ARIMA(DEMAND ~ TUESDAY.BOOK))
m %>% report()
m %>% accuracy()
m %>% select(ets) %>% report()
m %>% select(arima1) %>% report()
m %>% select(lr) %>% report()
m %>% select(arima2) %>% report()
m %>% select(arima3) %>% report()
m %>% select(arima2) %>%  residuals(type="regression") %>%
gg_tsdisplay(.resid, "partial", lag_max = 28)
m %>% select(arima3) %>%  residuals(type="regression") %>%
gg_tsdisplay(.resid, "partial", lag_max = 28)
m <- X%>% model(ets = ETS(DEMAND),
arima1 = ARIMA(DEMAND),
lr = TSLM(DEMAND ~ TUESDAY.BOOK),
arima20 = ARIMA(DEMAND ~ TUESDAY.BOOK + DOW.INDEX + pdq(0, 0, 1) + PDQ(0, 0, 0)),
arima21 = ARIMA(DEMAND ~ TUESDAY.BOOK + DOW.INDEX + pdq(1, 0, 0) + PDQ(0, 0, 0)),
arima22 = ARIMA(DEMAND ~ TUESDAY.BOOK + DOW.INDEX + pdq(1, 0, 1) + PDQ(0, 0, 0)),
arima23 = ARIMA(DEMAND ~ TUESDAY.BOOK + DOW.INDEX + pdq(3, 0, 3) + PDQ(0, 0, 0)),
arima30 = ARIMA(DEMAND ~ TUESDAY.BOOK + pdq(0, 0, 1) + PDQ(1, 0, 0)),
arima31 = ARIMA(DEMAND ~ TUESDAY.BOOK + pdq(2, 0, 0) + PDQ(1, 0, 0)),
arima32 = ARIMA(DEMAND ~ TUESDAY.BOOK + pdq(2, 0, 1) + PDQ(1, 0, 0)),
arima33 = ARIMA(DEMAND ~ TUESDAY.BOOK + pdq(0, 0, 1) + PDQ(1, 1, 0))
)
m %>% report()
m <- X%>% model(ets = ETS(DEMAND),
arima1 = ARIMA(DEMAND ~ pdq(1,0,0) + PDQ(2,0,0)),
lr = TSLM(DEMAND ~ TUESDAY.BOOK),
arima2 = ARIMA(DEMAND ~ TUESDAY.BOOK + DOW.INDEX + pdq(1,0,0) + PDQ(0, 0, 0)),
arima3 = ARIMA(DEMAND ~ TUESDAY.BOOK + pdq(2,0,1) + PDQ(1, 0, 0)))
m %>% report()
m %>% accuracy
m %>% select(arima2) %>%  residuals(type="regression") %>%
gg_tsdisplay(.resid, "partial", lag_max = 28)
m %>% select(arima3) %>%  residuals(type="regression") %>%
gg_tsdisplay(.resid, "partial", lag_max = 28)
m %>%
augment() %>%
features(.resid, ljung_box, lag = 18)
m %>%
augment() %>%
features(.resid, ljung_box, lag = 28)
Fcst.X
m %>% select(arima2) %>% forecast(new_data=Fcst.X)
fcst = m %>% select(arima2) %>% forecast(new_data=Fcst.X)
fcst = m %>% select(arima2) %>% forecast(new_data=Fcst.X)
fcst
Fcst.X
X
fcst = m %>% select(arima2) %>% forecast(new_data=Fcst.X)
fcst %>% hilo(level=c(80, 90))
fcst = m %>% select(arima2) %>% forecast(new_data=Fcst.X)
fcst %>% hilo(level=c(80, 90))  %>%
unpack_hilo()
fcst = m %>% select(arima2) %>% forecast(new_data=Fcst.X)
fcst %>% hilo(level=c(80, 90))  %>%
unpack_hilo("80%", "90%")
fcst = m %>% select(arima2) %>% forecast(new_data=Fcst.X)
fcst %>% hilo(level=c(80, 90))  %>%
unpack_hilo(c("80%", "90%"))
fcst %>% autoplot()
fcst %>% autoplot() + geom_line(X, mapping = aes(y = DEMAND))
?autoplot
fcst %>% autoplot(level=c(80, 90)) + geom_line(X, mapping = aes(y = DEMAND))
48*0.5*0.8
48*0.5*0.8+51
49*0.5*0.8+51
service_level = 0.67*(1877 - 60)
service_level
fcst
fcst %>% hilo(level=c(95))  %>%
unpack_hilo(c("95%")) %>% select(DATE, .mean, "95%_lower", "95%_upper")
fcst %>% hilo(level=c(95))  %>%
unpack_hilo(c("95%")) %>% select(DATE, .mean, "95%_lower", "95%_upper") %>% filter(DATE >= "2010-08-22")
library(fpp3)
library(tseries)
X <- read.csv("Booking Exercise.csv") %>%
mutate(DATE = ymd(DATE)) %>%
as_tsibble(index = DATE)
Fcst.X <- X %>% filter(DATE >= ymd("2010-08-18"))
X <- X %>% filter(DATE <= ymd("2010-08-17"))
X %>% autoplot(DEMAND)
X %>% features(DEMAND, unitroot_ndiffs)
X %>% features(DEMAND, unitroot_nsdiffs)
m <- X%>% model(ets = ETS(DEMAND),
arima1 = ARIMA(DEMAND),
lr = TSLM(DEMAND ~ TUESDAY.BOOK),
arima2 = ARIMA(DEMAND ~ TUESDAY.BOOK + DOW.INDEX + PDQ(0, 0, 0)),
arima3 = ARIMA(DEMAND ~ TUESDAY.BOOK))
m %>% report()
m %>% accuracy()
m %>% select(ets) %>% report()
m %>% select(arima1) %>% report()
m %>% select(lr) %>% report()
m %>% select(arima2) %>% report()
m %>% select(arima3) %>% report()
m %>% select(arima2) %>%  residuals(type="regression") %>%
gg_tsdisplay(.resid, "partial", lag_max = 28)
m %>% select(arima3) %>%  residuals(type="regression") %>%
gg_tsdisplay(.resid, "partial", lag_max = 28)
m <- X%>% model(ets = ETS(DEMAND),
arima1 = ARIMA(DEMAND),
lr = TSLM(DEMAND ~ TUESDAY.BOOK),
arima20 = ARIMA(DEMAND ~ TUESDAY.BOOK + DOW.INDEX + pdq(0, 0, 1) + PDQ(0, 0, 0)),
arima21 = ARIMA(DEMAND ~ TUESDAY.BOOK + DOW.INDEX + pdq(1, 0, 0) + PDQ(0, 0, 0)),
arima22 = ARIMA(DEMAND ~ TUESDAY.BOOK + DOW.INDEX + pdq(1, 0, 1) + PDQ(0, 0, 0)),
arima23 = ARIMA(DEMAND ~ TUESDAY.BOOK + DOW.INDEX + pdq(3, 0, 3) + PDQ(0, 0, 0)),
arima30 = ARIMA(DEMAND ~ TUESDAY.BOOK + pdq(0, 0, 1) + PDQ(1, 0, 0)),
arima31 = ARIMA(DEMAND ~ TUESDAY.BOOK + pdq(2, 0, 0) + PDQ(1, 0, 0)),
arima32 = ARIMA(DEMAND ~ TUESDAY.BOOK + pdq(2, 0, 1) + PDQ(1, 0, 0)),
arima33 = ARIMA(DEMAND ~ TUESDAY.BOOK + pdq(0, 0, 1) + PDQ(1, 1, 0))
)
m %>% report()
m %>% accuracy()
m <- X%>% model(ets = ETS(DEMAND),
arima1 = ARIMA(DEMAND ~ pdq(1,0,0) + PDQ(2,0,0)),
lr = TSLM(DEMAND ~ TUESDAY.BOOK),
arima2 = ARIMA(DEMAND ~ TUESDAY.BOOK + DOW.INDEX + pdq(1,0,0) + PDQ(0, 0, 0)),
arima3 = ARIMA(DEMAND ~ TUESDAY.BOOK + pdq(2,0,1) + PDQ(1, 0, 0)))
m %>% report()
m %>% accuracy
m %>%
augment() %>%
features(.resid, ljung_box, lag = 28)
fcst = m %>% select(arima2) %>% forecast(new_data=Fcst.X)
fcst %>% hilo(level=c(80, 90))  %>%
unpack_hilo(c("80%", "90%"))
fcst %>% hilo(level=c(95))  %>%
unpack_hilo(c("95%")) %>% select(DATE, .mean, "95%_lower", "95%_upper") %>% filter(DATE >= "2010-08-22")
service_level = (1877 - 60)/0.67
service_level
service_level = (1877 - 60)
service_level
fcst %>% hilo(level=c(33))  %>%
unpack_hilo(c("33%")) %>% select(DATE, .mean, "33%_lower", "33%_upper") %>% filter(DATE >= "2010-08-22")
library(mlogit)
# NESTED LOGIT
# choice of a heating mode
data("HC", package = "mlogit")
head(HC)
HC = mlogit.data(HC, varying = c(2:8, 10:16), choice="depvar", shape="wide")
# Setting the cooling costs to zero for the options without cooling
cooling.modes = HC$alt %in% c("gcc", "ecc", "erc", "hpc")
HC$icca[!cooling.modes] = 0
HC$occa[!cooling.modes] = 0
# Now we can do the estimation
HC.m1 = mlogit(depvar ~ occa + icca + och + ich, HC)
HC.m2 = mlogit(depvar ~ occa + icca + och + ich, HC, nests = list(cooling = c("ecc", "erc", "gcc", "hpc"), noncool = c("ec", "gc", "er")))
summary(HC.m1)
summary(HC.m2)
print(c(AIC(HC.m1),AIC(HC.m2)))
head(HC)
y = rnorm(1000)
hist(y)
x = rcauchy(1000)
hist(x)
# Generate Exponential â€“ R-code
rexpon = function(n, lambda=1) {
u = runif(n)
x = -1/lambda*log(u)
x
}
y = rexpon(2000,2)
hist(y, nclass=20, prob = TRUE)
lines(density(y))
# Generate Gumbel
rlogistic = function(n, mu=0, s=1) {
u = runif(n)
x = -log(-log(u))
x = s*x + mu
x
}
y = rlogistic(2000)
hist(y, nclass=20, prob = TRUE)
lines(density(y))
# Box-Muller R code
rnormal = function(n, mu=1, sigma=1) {
n1 = ceiling(n/2)
u1 = runif(n1)
u2 = runif(n1)
x1 = sqrt(-2*log(u1))*cos(2*pi*u2)
x2 = sqrt(-2*log(u1))*sin(2*pi*u2)
x = c(x1,x2)
y = x[1:n]*sigma + mu
y
}
y = rnormal(2000, mu=5, sigma=2)
hist(y, nclass=20, prob=T)
lines(density(y))
rinteger = function(prob=c(0.5,0.5), nint=length(p)) {
u1 = runif(1)
x = 0
ss = prob[1]
while (ss < u1) {
x = x+1
ss = ss + prob[x+1]
}
x
}
ppois(c(0:20), lambda=2)
dy = dpois(c(0:13), 2)
y = rep(0, 1000)
for(i in 1:1000) {
y[i]=rinteger(dy)
}
hist(y, nclass=20, prob=T)
hist(rpois(1000, 2), nclass=20, prob=T)
setwd("~/Mac/Downloads/UT/Spring22/Marketing/Project/Campaign-Outcome-Prediction/modeling")
knitr::opts_chunk$set(echo = FALSE)
train = read.csv('../data/train_transformed.csv')
# print()
head(train)
print(dim(train))
summary(train)
library(MCMCpack)
library(lme4)
library(MCMCpack)
library(lme4)
knitr::opts_chunk$set(echo = FALSE)
base_lm = glm(term_deposit_subscribed ~ job_type + marital + education + default + housing_loan + personal_loan + communication_type + prev_campaign_outcome + prev_campaign_contact + balance + last_contact_duration + num_contacts_in_campaign + num_contacts_prev_campaign + month + day_of_month + customer_age, data=train, family=binomial(link='logit'))
summary(base_lm)
base_lm = glm(term_deposit_subscribed ~ job_type + marital + education + default + housing_loan + personal_loan + communication_type + prev_campaign_outcome + prev_campaign_contact + balance + last_contact_duration + num_contacts_in_campaign + num_contacts_prev_campaign + month + day_of_month + customer_age, data=train, family=binomial(link='logit'))
summary(base_lm)
AIC(base_lm)
base_lm$fitted.values
base_lm = glm(term_deposit_subscribed ~ job_type + marital + education + default + housing_loan + personal_loan + communication_type + prev_campaign_outcome + prev_campaign_contact + balance + last_contact_duration + num_contacts_in_campaign + num_contacts_prev_campaign + month + day_of_month + customer_age, data=train, family=binomial(link='logit'))
summary(base_lm)
AIC(base_lm)
base_lm_train_pred = base_lm$fitted.values
base_lm_train_pred = as.numeric(base_lm_train_pred > 0.5)
table(base_lm_train_pred, train$term_deposit_subscribed)
sum(train$term_deposit_subscribed)
